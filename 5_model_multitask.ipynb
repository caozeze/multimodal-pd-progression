{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3bbd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import seaborn as sns\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines.utils import concordance_index\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LassoCV, LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr, f_oneway\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291ba459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 406 entries, 0 to 529\n",
      "Data columns (total 64 columns):\n",
      " #   Column                                   Non-Null Count  Dtype  \n",
      "---  ------                                   --------------  -----  \n",
      " 0   age                                      406 non-null    float64\n",
      " 1   fampd                                    406 non-null    int64  \n",
      " 2   race_black                               406 non-null    bool   \n",
      " 3   race_asian                               406 non-null    bool   \n",
      " 4   race_other                               406 non-null    bool   \n",
      " 5   sex                                      406 non-null    int64  \n",
      " 6   educyrs                                  406 non-null    float64\n",
      " 7   subgroup_gba                             406 non-null    bool   \n",
      " 8   subgroup_lrrk2                           406 non-null    bool   \n",
      " 9   subgroup_prkn                            406 non-null    bool   \n",
      " 10  apoe_e4                                  406 non-null    float64\n",
      " 11  csfsaa_positive_lbd_like                 406 non-null    bool   \n",
      " 12  csfsaa_positive_msa_like                 406 non-null    bool   \n",
      " 13  csfsaa_inconclusive                      406 non-null    bool   \n",
      " 14  urate                                    406 non-null    float64\n",
      " 15  mia_caudate_l                            406 non-null    float64\n",
      " 16  mia_caudate_r                            406 non-null    float64\n",
      " 17  mia_putamen_l                            406 non-null    float64\n",
      " 18  mia_putamen_r                            406 non-null    float64\n",
      " 19  duration_yrs                             406 non-null    float64\n",
      " 20  ledd                                     406 non-null    float64\n",
      " 21  mseadlg                                  406 non-null    float64\n",
      " 22  hy                                       406 non-null    float64\n",
      " 23  domside_left                             406 non-null    bool   \n",
      " 24  domside_symmetric                        406 non-null    bool   \n",
      " 25  hvltrdly                                 406 non-null    float64\n",
      " 26  lns                                      406 non-null    float64\n",
      " 27  vltanim                                  406 non-null    float64\n",
      " 28  quip                                     406 non-null    float64\n",
      " 29  ess                                      406 non-null    float64\n",
      " 30  pigd                                     406 non-null    float64\n",
      " 31  updrs2_score                             406 non-null    float64\n",
      " 32  updrs3_score                             406 non-null    float64\n",
      " 33  sdmtotal                                 406 non-null    float64\n",
      " 34  stai                                     406 non-null    float64\n",
      " 35  moca                                     406 non-null    float64\n",
      " 36  rem                                      406 non-null    float64\n",
      " 37  gds                                      406 non-null    float64\n",
      " 38  bmi                                      406 non-null    float64\n",
      " 39  updrs1_score                             406 non-null    float64\n",
      " 40  bjlot                                    406 non-null    float64\n",
      " 41  scopa                                    406 non-null    float64\n",
      " 42  upsit_pctl                               406 non-null    float64\n",
      " 43  mri_pc1                                  406 non-null    float64\n",
      " 44  mri_pc2                                  406 non-null    float64\n",
      " 45  mri_pc3                                  406 non-null    float64\n",
      " 46  mri_pc4                                  406 non-null    float64\n",
      " 47  mri_pc5                                  406 non-null    float64\n",
      " 48  mri_pc6                                  406 non-null    float64\n",
      " 49  mri_pc7                                  406 non-null    float64\n",
      " 50  mri_pc8                                  406 non-null    float64\n",
      " 51  mri_pc9                                  406 non-null    float64\n",
      " 52  mri_pc10                                 406 non-null    float64\n",
      " 53  moca_slope_iqr_cleaned                   406 non-null    float64\n",
      " 54  scopa_slope_iqr_cleaned                  406 non-null    float64\n",
      " 55  contralateral_putamen_slope_iqr_cleaned  406 non-null    float64\n",
      " 56  mia_putamen_mean_slope_iqr_cleaned       406 non-null    float64\n",
      " 57  updrs1_score_slope_iqr_cleaned           406 non-null    float64\n",
      " 58  updrs2_score_slope_iqr_cleaned           406 non-null    float64\n",
      " 59  updrs3_score_slope_iqr_cleaned           406 non-null    float64\n",
      " 60  updrs_totscore_slope_iqr_cleaned         406 non-null    float64\n",
      " 61  time_to_hy3_plus                         406 non-null    float64\n",
      " 62  event_occurred                           406 non-null    float64\n",
      " 63  censored                                 406 non-null    float64\n",
      "dtypes: bool(11), float64(51), int64(2)\n",
      "memory usage: 175.6 KB\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "data_path = './data/'\n",
    "curated_mri_vif_df = pd.read_csv(data_path + '3_baseline_vif.csv')\n",
    "curated_mri_vif_df = curated_mri_vif_df.dropna()\n",
    "curated_mri_vif_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a40a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ RQ4 CONFIGURATION: MTL WITH CORRECTED COMBINED SCORE\n",
      "======================================================================\n",
      "ğŸ¯ Targets: moca_slope_iqr_cleaned + time_to_hy3_plus\n",
      "ğŸ“Š Total features: 53\n",
      "âš™ï¸ Experiments: 100\n",
      "âœ… Fixed components defined!\n",
      "ğŸ¯ Starting Fixed MTL Experiment\n",
      "Data shape: (406, 64)\n",
      "ğŸš€ STARTING FIXED MTL EXPERIMENT\n",
      "================================================================================\n",
      "ğŸ“Š Experiments: 100\n",
      "ğŸ“… Epochs per experiment: 200\n",
      "\n",
      "ğŸ¯ BASELINE COMPARISON:\n",
      "   â€¢ Regression RÂ²: 0.210\n",
      "   â€¢ Survival C-index: 0.830\n",
      "\n",
      "ğŸ”§ FIXED MTL FEATURES:\n",
      "   â€¢ Corrected data processing (no leakage)\n",
      "   â€¢ Balanced loss function\n",
      "   â€¢ Normalized combined score\n",
      "\n",
      "--- Experiment 1/100 ---\n",
      "Params: lr=0.0001, bs=32, hd=256, Î±r=0.7, Î±s=0.5\n",
      "ğŸ“Š RÂ²: 0.0781 | C-idx: 0.7483\n",
      "ğŸ“Š RÂ² Î”: -0.1319 | C-idx Î”: -0.0817\n",
      "ğŸ“Š Combined (norm): 0.2873 | Combined (simple): 0.8264\n",
      "ğŸ“Š Epochs: 200\n",
      "ğŸ’¾ New best model saved!\n",
      "\n",
      "--- Experiment 2/100 ---\n",
      "Params: lr=0.0001, bs=32, hd=64, Î±r=0.5, Î±s=1.5\n",
      "ğŸ“Š RÂ²: 0.0387 | C-idx: 0.8160\n",
      "ğŸ“Š RÂ² Î”: -0.1713 | C-idx Î”: -0.0140\n",
      "ğŸ“Š Combined (norm): 0.3354 | Combined (simple): 0.8547\n",
      "ğŸ“Š Epochs: 200\n",
      "ğŸ’¾ New best model saved!\n",
      "\n",
      "--- Experiment 3/100 ---\n",
      "Params: lr=0.0001, bs=32, hd=256, Î±r=1.0, Î±s=0.7\n",
      "ğŸ“Š RÂ²: 0.0013 | C-idx: 0.7372\n",
      "ğŸ“Š RÂ² Î”: -0.2087 | C-idx Î”: -0.0928\n",
      "ğŸ“Š Combined (norm): 0.2378 | Combined (simple): 0.7385\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 4/100 ---\n",
      "Params: lr=0.002, bs=32, hd=128, Î±r=0.7, Î±s=1.5\n",
      "ğŸ“Š RÂ²: -0.0000 | C-idx: 0.8243\n",
      "ğŸ“Š RÂ² Î”: -0.2100 | C-idx Î”: -0.0057\n",
      "ğŸ“Š Combined (norm): 0.3243 | Combined (simple): 0.8243\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 5/100 ---\n",
      "Params: lr=0.001, bs=128, hd=256, Î±r=0.7, Î±s=1.5\n",
      "ğŸ“Š RÂ²: -0.1020 | C-idx: 0.7732\n",
      "ğŸ“Š RÂ² Î”: -0.3120 | C-idx Î”: -0.0568\n",
      "ğŸ“Š Combined (norm): 0.2732 | Combined (simple): 0.6711\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 6/100 ---\n",
      "Params: lr=0.0005, bs=32, hd=128, Î±r=0.7, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.0875 | C-idx: 0.7497\n",
      "ğŸ“Š RÂ² Î”: -0.1225 | C-idx Î”: -0.0803\n",
      "ğŸ“Š Combined (norm): 0.2934 | Combined (simple): 0.8372\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 7/100 ---\n",
      "Params: lr=0.0001, bs=128, hd=256, Î±r=0.5, Î±s=1.5\n",
      "ğŸ“Š RÂ²: 0.0802 | C-idx: 0.7911\n",
      "ğŸ“Š RÂ² Î”: -0.1298 | C-idx Î”: -0.0389\n",
      "ğŸ“Š Combined (norm): 0.3312 | Combined (simple): 0.8713\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 8/100 ---\n",
      "Params: lr=0.001, bs=32, hd=64, Î±r=0.3, Î±s=0.7\n",
      "ğŸ“Š RÂ²: 0.0237 | C-idx: 0.7026\n",
      "ğŸ“Š RÂ² Î”: -0.1863 | C-idx Î”: -0.1274\n",
      "ğŸ“Š Combined (norm): 0.2145 | Combined (simple): 0.7263\n",
      "ğŸ“Š Epochs: 148\n",
      "\n",
      "--- Experiment 9/100 ---\n",
      "Params: lr=0.002, bs=64, hd=256, Î±r=1.0, Î±s=0.5\n",
      "ğŸ“Š RÂ²: -1.9631 | C-idx: 0.8313\n",
      "ğŸ“Š RÂ² Î”: -2.1731 | C-idx Î”: +0.0013\n",
      "ğŸ“Š Combined (norm): 0.3313 | Combined (simple): -1.1318\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 10/100 ---\n",
      "Params: lr=0.001, bs=32, hd=64, Î±r=1.5, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.0017 | C-idx: 0.7469\n",
      "ğŸ“Š RÂ² Î”: -0.2083 | C-idx Î”: -0.0831\n",
      "ğŸ“Š Combined (norm): 0.2477 | Combined (simple): 0.7485\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 11/100 ---\n",
      "Params: lr=0.0001, bs=64, hd=128, Î±r=1.5, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.0283 | C-idx: 0.7842\n",
      "ğŸ“Š RÂ² Î”: -0.1817 | C-idx Î”: -0.0458\n",
      "ğŸ“Š Combined (norm): 0.2984 | Combined (simple): 0.8125\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 12/100 ---\n",
      "Params: lr=0.001, bs=64, hd=64, Î±r=1.0, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.0985 | C-idx: 0.7234\n",
      "ğŸ“Š RÂ² Î”: -0.1115 | C-idx Î”: -0.1066\n",
      "ğŸ“Š Combined (norm): 0.2726 | Combined (simple): 0.8219\n",
      "ğŸ“Š Epochs: 198\n",
      "\n",
      "--- Experiment 13/100 ---\n",
      "Params: lr=0.001, bs=32, hd=256, Î±r=1.5, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.1378 | C-idx: 0.7510\n",
      "ğŸ“Š RÂ² Î”: -0.0722 | C-idx Î”: -0.0790\n",
      "ğŸ“Š Combined (norm): 0.3199 | Combined (simple): 0.8888\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 14/100 ---\n",
      "Params: lr=0.0005, bs=32, hd=64, Î±r=1.5, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.1481 | C-idx: 0.7718\n",
      "ğŸ“Š RÂ² Î”: -0.0619 | C-idx Î”: -0.0582\n",
      "ğŸ“Š Combined (norm): 0.3458 | Combined (simple): 0.9199\n",
      "ğŸ“Š Epochs: 200\n",
      "ğŸ’¾ New best model saved!\n",
      "\n",
      "--- Experiment 15/100 ---\n",
      "Params: lr=0.0005, bs=64, hd=256, Î±r=1.5, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.1631 | C-idx: 0.7635\n",
      "ğŸ“Š RÂ² Î”: -0.0469 | C-idx Î”: -0.0665\n",
      "ğŸ“Š Combined (norm): 0.3451 | Combined (simple): 0.9266\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 16/100 ---\n",
      "Params: lr=0.002, bs=32, hd=64, Î±r=0.5, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.0636 | C-idx: 0.7123\n",
      "ğŸ“Š RÂ² Î”: -0.1464 | C-idx Î”: -0.1177\n",
      "ğŸ“Š Combined (norm): 0.2441 | Combined (simple): 0.7759\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 17/100 ---\n",
      "Params: lr=0.0001, bs=32, hd=64, Î±r=0.3, Î±s=0.7\n",
      "ğŸ“Š RÂ²: 0.0142 | C-idx: 0.7925\n",
      "ğŸ“Š RÂ² Î”: -0.1958 | C-idx Î”: -0.0375\n",
      "ğŸ“Š Combined (norm): 0.2996 | Combined (simple): 0.8067\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 18/100 ---\n",
      "Params: lr=0.002, bs=32, hd=128, Î±r=1.0, Î±s=0.5\n",
      "ğŸ“Š RÂ²: -0.0002 | C-idx: 0.8271\n",
      "ğŸ“Š RÂ² Î”: -0.2102 | C-idx Î”: -0.0029\n",
      "ğŸ“Š Combined (norm): 0.3271 | Combined (simple): 0.8269\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 19/100 ---\n",
      "Params: lr=0.002, bs=128, hd=64, Î±r=0.3, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.0677 | C-idx: 0.7510\n",
      "ğŸ“Š RÂ² Î”: -0.1423 | C-idx Î”: -0.0790\n",
      "ğŸ“Š Combined (norm): 0.2849 | Combined (simple): 0.8188\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 20/100 ---\n",
      "Params: lr=0.002, bs=32, hd=128, Î±r=0.5, Î±s=0.7\n",
      "ğŸ“Š RÂ²: 0.1057 | C-idx: 0.7206\n",
      "ğŸ“Š RÂ² Î”: -0.1043 | C-idx Î”: -0.1094\n",
      "ğŸ“Š Combined (norm): 0.2734 | Combined (simple): 0.8263\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 21/100 ---\n",
      "Params: lr=0.0001, bs=32, hd=64, Î±r=0.5, Î±s=1.0\n",
      "ğŸ“Š RÂ²: -0.0011 | C-idx: 0.8243\n",
      "ğŸ“Š RÂ² Î”: -0.2111 | C-idx Î”: -0.0057\n",
      "ğŸ“Š Combined (norm): 0.3243 | Combined (simple): 0.8233\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 22/100 ---\n",
      "Params: lr=0.002, bs=32, hd=128, Î±r=0.7, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.0410 | C-idx: 0.7109\n",
      "ğŸ“Š RÂ² Î”: -0.1690 | C-idx Î”: -0.1191\n",
      "ğŸ“Š Combined (norm): 0.2314 | Combined (simple): 0.7519\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 23/100 ---\n",
      "Params: lr=0.0005, bs=32, hd=256, Î±r=1.5, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.0033 | C-idx: 0.7649\n",
      "ğŸ“Š RÂ² Î”: -0.2067 | C-idx Î”: -0.0651\n",
      "ğŸ“Š Combined (norm): 0.2665 | Combined (simple): 0.7681\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 24/100 ---\n",
      "Params: lr=0.0001, bs=32, hd=64, Î±r=1.5, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.0821 | C-idx: 0.7676\n",
      "ğŸ“Š RÂ² Î”: -0.1279 | C-idx Î”: -0.0624\n",
      "ğŸ“Š Combined (norm): 0.3087 | Combined (simple): 0.8497\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 25/100 ---\n",
      "Params: lr=0.002, bs=128, hd=256, Î±r=1.5, Î±s=1.5\n",
      "ğŸ“Š RÂ²: 0.1195 | C-idx: 0.7427\n",
      "ğŸ“Š RÂ² Î”: -0.0905 | C-idx Î”: -0.0873\n",
      "ğŸ“Š Combined (norm): 0.3025 | Combined (simple): 0.8623\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 26/100 ---\n",
      "Params: lr=0.002, bs=32, hd=256, Î±r=0.7, Î±s=1.0\n",
      "ğŸ“Š RÂ²: -0.0560 | C-idx: 0.6196\n",
      "ğŸ“Š RÂ² Î”: -0.2660 | C-idx Î”: -0.2104\n",
      "ğŸ“Š Combined (norm): 0.1196 | Combined (simple): 0.5637\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 27/100 ---\n",
      "Params: lr=0.0005, bs=128, hd=128, Î±r=0.5, Î±s=0.7\n",
      "ğŸ“Š RÂ²: 0.0576 | C-idx: 0.7704\n",
      "ğŸ“Š RÂ² Î”: -0.1524 | C-idx Î”: -0.0596\n",
      "ğŸ“Š Combined (norm): 0.2992 | Combined (simple): 0.8280\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 28/100 ---\n",
      "Params: lr=0.001, bs=128, hd=256, Î±r=1.5, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.0604 | C-idx: 0.7649\n",
      "ğŸ“Š RÂ² Î”: -0.1496 | C-idx Î”: -0.0651\n",
      "ğŸ“Š Combined (norm): 0.2950 | Combined (simple): 0.8252\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 29/100 ---\n",
      "Params: lr=0.0005, bs=64, hd=128, Î±r=1.5, Î±s=0.5\n",
      "ğŸ“Š RÂ²: 0.0407 | C-idx: 0.7331\n",
      "ğŸ“Š RÂ² Î”: -0.1693 | C-idx Î”: -0.0969\n",
      "ğŸ“Š Combined (norm): 0.2534 | Combined (simple): 0.7738\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 30/100 ---\n",
      "Params: lr=0.0001, bs=128, hd=128, Î±r=0.7, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.1152 | C-idx: 0.8050\n",
      "ğŸ“Š RÂ² Î”: -0.0948 | C-idx Î”: -0.0250\n",
      "ğŸ“Š Combined (norm): 0.3626 | Combined (simple): 0.9202\n",
      "ğŸ“Š Epochs: 200\n",
      "ğŸ’¾ New best model saved!\n",
      "\n",
      "--- Experiment 31/100 ---\n",
      "Params: lr=0.002, bs=128, hd=64, Î±r=0.3, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.0363 | C-idx: 0.7455\n",
      "ğŸ“Š RÂ² Î”: -0.1737 | C-idx Î”: -0.0845\n",
      "ğŸ“Š Combined (norm): 0.2637 | Combined (simple): 0.7819\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 32/100 ---\n",
      "Params: lr=0.0001, bs=64, hd=128, Î±r=0.3, Î±s=0.7\n",
      "ğŸ“Š RÂ²: 0.0415 | C-idx: 0.8022\n",
      "ğŸ“Š RÂ² Î”: -0.1685 | C-idx Î”: -0.0278\n",
      "ğŸ“Š Combined (norm): 0.3229 | Combined (simple): 0.8437\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 33/100 ---\n",
      "Params: lr=0.0005, bs=32, hd=64, Î±r=1.0, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.1229 | C-idx: 0.7773\n",
      "ğŸ“Š RÂ² Î”: -0.0871 | C-idx Î”: -0.0527\n",
      "ğŸ“Š Combined (norm): 0.3388 | Combined (simple): 0.9003\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 34/100 ---\n",
      "Params: lr=0.0001, bs=64, hd=64, Î±r=1.0, Î±s=0.5\n",
      "ğŸ“Š RÂ²: -0.0065 | C-idx: 0.8257\n",
      "ğŸ“Š RÂ² Î”: -0.2165 | C-idx Î”: -0.0043\n",
      "ğŸ“Š Combined (norm): 0.3257 | Combined (simple): 0.8192\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 35/100 ---\n",
      "Params: lr=0.0001, bs=128, hd=64, Î±r=1.0, Î±s=0.7\n",
      "ğŸ“Š RÂ²: 0.0119 | C-idx: 0.8188\n",
      "ğŸ“Š RÂ² Î”: -0.1981 | C-idx Î”: -0.0112\n",
      "ğŸ“Š Combined (norm): 0.3247 | Combined (simple): 0.8307\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 36/100 ---\n",
      "Params: lr=0.0001, bs=32, hd=128, Î±r=0.5, Î±s=1.5\n",
      "ğŸ“Š RÂ²: 0.0761 | C-idx: 0.7607\n",
      "ğŸ“Š RÂ² Î”: -0.1339 | C-idx Î”: -0.0693\n",
      "ğŸ“Š Combined (norm): 0.2988 | Combined (simple): 0.8368\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 37/100 ---\n",
      "Params: lr=0.002, bs=128, hd=256, Î±r=0.3, Î±s=1.0\n",
      "ğŸ“Š RÂ²: -0.0013 | C-idx: 0.7441\n",
      "ğŸ“Š RÂ² Î”: -0.2113 | C-idx Î”: -0.0859\n",
      "ğŸ“Š Combined (norm): 0.2441 | Combined (simple): 0.7428\n",
      "ğŸ“Š Epochs: 192\n",
      "\n",
      "--- Experiment 38/100 ---\n",
      "Params: lr=0.001, bs=64, hd=64, Î±r=0.7, Î±s=1.5\n",
      "ğŸ“Š RÂ²: 0.0608 | C-idx: 0.7372\n",
      "ğŸ“Š RÂ² Î”: -0.1492 | C-idx Î”: -0.0928\n",
      "ğŸ“Š Combined (norm): 0.2676 | Combined (simple): 0.7981\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 39/100 ---\n",
      "Params: lr=0.002, bs=128, hd=128, Î±r=1.5, Î±s=0.5\n",
      "ğŸ“Š RÂ²: 0.0160 | C-idx: 0.7676\n",
      "ğŸ“Š RÂ² Î”: -0.1940 | C-idx Î”: -0.0624\n",
      "ğŸ“Š Combined (norm): 0.2756 | Combined (simple): 0.7836\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 40/100 ---\n",
      "Params: lr=0.0001, bs=64, hd=128, Î±r=0.5, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.0249 | C-idx: 0.7856\n",
      "ğŸ“Š RÂ² Î”: -0.1851 | C-idx Î”: -0.0444\n",
      "ğŸ“Š Combined (norm): 0.2980 | Combined (simple): 0.8105\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 41/100 ---\n",
      "Params: lr=0.0005, bs=128, hd=64, Î±r=0.7, Î±s=1.5\n",
      "ğŸ“Š RÂ²: 0.0442 | C-idx: 0.7220\n",
      "ğŸ“Š RÂ² Î”: -0.1658 | C-idx Î”: -0.1080\n",
      "ğŸ“Š Combined (norm): 0.2441 | Combined (simple): 0.7662\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 42/100 ---\n",
      "Params: lr=0.0005, bs=32, hd=64, Î±r=0.5, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.0281 | C-idx: 0.7746\n",
      "ğŸ“Š RÂ² Î”: -0.1819 | C-idx Î”: -0.0554\n",
      "ğŸ“Š Combined (norm): 0.2886 | Combined (simple): 0.8026\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 43/100 ---\n",
      "Params: lr=0.002, bs=32, hd=64, Î±r=0.3, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.0809 | C-idx: 0.6943\n",
      "ğŸ“Š RÂ² Î”: -0.1291 | C-idx Î”: -0.1357\n",
      "ğŸ“Š Combined (norm): 0.2348 | Combined (simple): 0.7752\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 44/100 ---\n",
      "Params: lr=0.0001, bs=64, hd=64, Î±r=1.0, Î±s=2.0\n",
      "ğŸ“Š RÂ²: -0.0166 | C-idx: 0.8257\n",
      "ğŸ“Š RÂ² Î”: -0.2266 | C-idx Î”: -0.0043\n",
      "ğŸ“Š Combined (norm): 0.3257 | Combined (simple): 0.8091\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 45/100 ---\n",
      "Params: lr=0.002, bs=64, hd=64, Î±r=0.7, Î±s=1.5\n",
      "ğŸ“Š RÂ²: 0.0378 | C-idx: 0.6860\n",
      "ğŸ“Š RÂ² Î”: -0.1722 | C-idx Î”: -0.1440\n",
      "ğŸ“Š Combined (norm): 0.2049 | Combined (simple): 0.7239\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 46/100 ---\n",
      "Params: lr=0.0005, bs=64, hd=128, Î±r=0.7, Î±s=1.5\n",
      "ğŸ“Š RÂ²: 0.0364 | C-idx: 0.7607\n",
      "ğŸ“Š RÂ² Î”: -0.1736 | C-idx Î”: -0.0693\n",
      "ğŸ“Š Combined (norm): 0.2789 | Combined (simple): 0.7971\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 47/100 ---\n",
      "Params: lr=0.0005, bs=32, hd=128, Î±r=1.0, Î±s=0.7\n",
      "ğŸ“Š RÂ²: 0.0819 | C-idx: 0.7566\n",
      "ğŸ“Š RÂ² Î”: -0.1281 | C-idx Î”: -0.0734\n",
      "ğŸ“Š Combined (norm): 0.2975 | Combined (simple): 0.8384\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 48/100 ---\n",
      "Params: lr=0.0001, bs=128, hd=128, Î±r=1.0, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.0103 | C-idx: 0.8160\n",
      "ğŸ“Š RÂ² Î”: -0.1997 | C-idx Î”: -0.0140\n",
      "ğŸ“Š Combined (norm): 0.3212 | Combined (simple): 0.8263\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 49/100 ---\n",
      "Params: lr=0.0005, bs=64, hd=128, Î±r=1.0, Î±s=0.3\n",
      "ğŸ“Š RÂ²: 0.0538 | C-idx: 0.7483\n",
      "ğŸ“Š RÂ² Î”: -0.1562 | C-idx Î”: -0.0817\n",
      "ğŸ“Š Combined (norm): 0.2752 | Combined (simple): 0.8021\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 50/100 ---\n",
      "Params: lr=0.0001, bs=64, hd=256, Î±r=1.5, Î±s=2.0\n",
      "ğŸ“Š RÂ²: 0.0808 | C-idx: 0.7953\n",
      "ğŸ“Š RÂ² Î”: -0.1292 | C-idx Î”: -0.0347\n",
      "ğŸ“Š Combined (norm): 0.3357 | Combined (simple): 0.8761\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 51/100 ---\n",
      "Params: lr=0.0001, bs=64, hd=128, Î±r=0.7, Î±s=0.5\n",
      "ğŸ“Š RÂ²: 0.0340 | C-idx: 0.7815\n",
      "ğŸ“Š RÂ² Î”: -0.1760 | C-idx Î”: -0.0485\n",
      "ğŸ“Š Combined (norm): 0.2984 | Combined (simple): 0.8154\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 52/100 ---\n",
      "Params: lr=0.001, bs=32, hd=128, Î±r=0.3, Î±s=2.0\n",
      "ğŸ“Š RÂ²: 0.0353 | C-idx: 0.7469\n",
      "ğŸ“Š RÂ² Î”: -0.1747 | C-idx Î”: -0.0831\n",
      "ğŸ“Š Combined (norm): 0.2645 | Combined (simple): 0.7821\n",
      "ğŸ“Š Epochs: 195\n",
      "\n",
      "--- Experiment 53/100 ---\n",
      "Params: lr=0.0005, bs=32, hd=64, Î±r=1.5, Î±s=0.5\n",
      "ğŸ“Š RÂ²: 0.1148 | C-idx: 0.7427\n",
      "ğŸ“Š RÂ² Î”: -0.0952 | C-idx Î”: -0.0873\n",
      "ğŸ“Š Combined (norm): 0.3001 | Combined (simple): 0.8575\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 54/100 ---\n",
      "Params: lr=0.001, bs=64, hd=64, Î±r=1.5, Î±s=1.5\n",
      "ğŸ“Š RÂ²: 0.0462 | C-idx: 0.7012\n",
      "ğŸ“Š RÂ² Î”: -0.1638 | C-idx Î”: -0.1288\n",
      "ğŸ“Š Combined (norm): 0.2244 | Combined (simple): 0.7475\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 55/100 ---\n",
      "Params: lr=0.002, bs=64, hd=256, Î±r=0.5, Î±s=0.3\n",
      "ğŸ“Š RÂ²: -0.0626 | C-idx: 0.5934\n",
      "ğŸ“Š RÂ² Î”: -0.2726 | C-idx Î”: -0.2366\n",
      "ğŸ“Š Combined (norm): 0.0934 | Combined (simple): 0.5308\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 56/100 ---\n",
      "Params: lr=0.002, bs=128, hd=128, Î±r=0.3, Î±s=1.5\n",
      "ğŸ“Š RÂ²: 0.0269 | C-idx: 0.7331\n",
      "ğŸ“Š RÂ² Î”: -0.1831 | C-idx Î”: -0.0969\n",
      "ğŸ“Š Combined (norm): 0.2465 | Combined (simple): 0.7599\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 57/100 ---\n",
      "Params: lr=0.001, bs=128, hd=128, Î±r=0.5, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.0515 | C-idx: 0.7621\n",
      "ğŸ“Š RÂ² Î”: -0.1585 | C-idx Î”: -0.0679\n",
      "ğŸ“Š Combined (norm): 0.2878 | Combined (simple): 0.8136\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 58/100 ---\n",
      "Params: lr=0.001, bs=32, hd=64, Î±r=0.7, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.0321 | C-idx: 0.7607\n",
      "ğŸ“Š RÂ² Î”: -0.1779 | C-idx Î”: -0.0693\n",
      "ğŸ“Š Combined (norm): 0.2768 | Combined (simple): 0.7928\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 59/100 ---\n",
      "Params: lr=0.001, bs=32, hd=128, Î±r=0.5, Î±s=0.7\n",
      "ğŸ“Š RÂ²: 0.0375 | C-idx: 0.6999\n",
      "ğŸ“Š RÂ² Î”: -0.1725 | C-idx Î”: -0.1301\n",
      "ğŸ“Š Combined (norm): 0.2186 | Combined (simple): 0.7374\n",
      "ğŸ“Š Epochs: 125\n",
      "\n",
      "--- Experiment 60/100 ---\n",
      "Params: lr=0.0001, bs=32, hd=256, Î±r=1.0, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.0724 | C-idx: 0.7552\n",
      "ğŸ“Š RÂ² Î”: -0.1376 | C-idx Î”: -0.0748\n",
      "ğŸ“Š Combined (norm): 0.2914 | Combined (simple): 0.8276\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 61/100 ---\n",
      "Params: lr=0.001, bs=32, hd=128, Î±r=0.5, Î±s=0.7\n",
      "ğŸ“Š RÂ²: 0.0282 | C-idx: 0.7414\n",
      "ğŸ“Š RÂ² Î”: -0.1818 | C-idx Î”: -0.0886\n",
      "ğŸ“Š Combined (norm): 0.2554 | Combined (simple): 0.7695\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 62/100 ---\n",
      "Params: lr=0.002, bs=64, hd=128, Î±r=0.7, Î±s=0.5\n",
      "ğŸ“Š RÂ²: 0.1140 | C-idx: 0.6680\n",
      "ğŸ“Š RÂ² Î”: -0.0960 | C-idx Î”: -0.1620\n",
      "ğŸ“Š Combined (norm): 0.2251 | Combined (simple): 0.7821\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 63/100 ---\n",
      "Params: lr=0.0005, bs=128, hd=128, Î±r=0.7, Î±s=2.0\n",
      "ğŸ“Š RÂ²: 0.0444 | C-idx: 0.7718\n",
      "ğŸ“Š RÂ² Î”: -0.1656 | C-idx Î”: -0.0582\n",
      "ğŸ“Š Combined (norm): 0.2940 | Combined (simple): 0.8162\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 64/100 ---\n",
      "Params: lr=0.0005, bs=64, hd=64, Î±r=1.5, Î±s=0.5\n",
      "ğŸ“Š RÂ²: 0.1457 | C-idx: 0.7593\n",
      "ğŸ“Š RÂ² Î”: -0.0643 | C-idx Î”: -0.0707\n",
      "ğŸ“Š Combined (norm): 0.3322 | Combined (simple): 0.9050\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 65/100 ---\n",
      "Params: lr=0.0001, bs=32, hd=256, Î±r=0.7, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.0445 | C-idx: 0.7372\n",
      "ğŸ“Š RÂ² Î”: -0.1655 | C-idx Î”: -0.0928\n",
      "ğŸ“Š Combined (norm): 0.2595 | Combined (simple): 0.7817\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 66/100 ---\n",
      "Params: lr=0.002, bs=32, hd=64, Î±r=1.0, Î±s=1.0\n",
      "ğŸ“Š RÂ²: 0.0251 | C-idx: 0.7690\n",
      "ğŸ“Š RÂ² Î”: -0.1849 | C-idx Î”: -0.0610\n",
      "ğŸ“Š Combined (norm): 0.2816 | Combined (simple): 0.7941\n",
      "ğŸ“Š Epochs: 184\n",
      "\n",
      "--- Experiment 67/100 ---\n",
      "Params: lr=0.0005, bs=32, hd=256, Î±r=1.0, Î±s=1.5\n",
      "ğŸ“Š RÂ²: -15.2960 | C-idx: 0.8326\n",
      "ğŸ“Š RÂ² Î”: -15.5060 | C-idx Î”: +0.0026\n",
      "ğŸ“Š Combined (norm): 0.3326 | Combined (simple): -14.4634\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 68/100 ---\n",
      "Params: lr=0.001, bs=128, hd=256, Î±r=0.3, Î±s=1.5\n",
      "ğŸ“Š RÂ²: -0.0562 | C-idx: 0.7607\n",
      "ğŸ“Š RÂ² Î”: -0.2662 | C-idx Î”: -0.0693\n",
      "ğŸ“Š Combined (norm): 0.2607 | Combined (simple): 0.7045\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 69/100 ---\n",
      "Params: lr=0.0005, bs=32, hd=256, Î±r=0.3, Î±s=0.5\n",
      "ğŸ“Š RÂ²: -295.0868 | C-idx: 0.8437\n",
      "ğŸ“Š RÂ² Î”: -295.2968 | C-idx Î”: +0.0137\n",
      "ğŸ“Š Combined (norm): 0.3437 | Combined (simple): -294.2431\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 70/100 ---\n",
      "Params: lr=0.0005, bs=64, hd=256, Î±r=0.7, Î±s=2.0\n",
      "ğŸ“Š RÂ²: 0.0460 | C-idx: 0.7469\n",
      "ğŸ“Š RÂ² Î”: -0.1640 | C-idx Î”: -0.0831\n",
      "ğŸ“Š Combined (norm): 0.2699 | Combined (simple): 0.7929\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 71/100 ---\n",
      "Params: lr=0.0001, bs=128, hd=64, Î±r=0.5, Î±s=0.7\n",
      "ğŸ“Š RÂ²: 0.0440 | C-idx: 0.8216\n",
      "ğŸ“Š RÂ² Î”: -0.1660 | C-idx Î”: -0.0084\n",
      "ğŸ“Š Combined (norm): 0.3436 | Combined (simple): 0.8655\n",
      "ğŸ“Š Epochs: 200\n",
      "\n",
      "--- Experiment 72/100 ---\n",
      "Params: lr=0.002, bs=32, hd=128, Î±r=0.7, Î±s=2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from lifelines.utils import concordance_index\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Loading\n",
    "data_path = './data/'\n",
    "curated_mri_vif_df = pd.read_csv(data_path + '3_baseline_vif.csv')\n",
    "curated_mri_vif_df = curated_mri_vif_df.dropna()\n",
    "curated_mri_vif_df.info()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from lifelines.utils import concordance_index\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "#\n",
    "# RESEARCH QUESTION 4: MULTI-TASK LEARNING (MTL) ANALYSIS - COMPLETE VERSION\n",
    "# WITH TRAINING DATA PRESERVATION FOR ACCURATE EVALUATION\n",
    "#\n",
    "# Author: GitHub Copilot (Complete Version)\n",
    "# Date: 9 September 2025\n",
    "#\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ”§ RQ4 CONFIGURATION: COMPLETE MTL WITH DATA PRESERVATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define modality groups\n",
    "modality_groups = {\n",
    "    'demographic_clinical': [\n",
    "        'age', 'fampd', 'race_black', 'race_asian', 'race_other', 'sex',\n",
    "        'educyrs', 'duration_yrs', 'ledd', 'mseadlg', 'hy', 'domside_left', \n",
    "        'domside_symmetric', 'hvltrdly', 'lns', 'vltanim', 'quip', 'ess', 'pigd', \n",
    "        'updrs2_score', 'updrs3_score', 'sdmtotal', 'stai', 'moca', 'rem', 'gds', \n",
    "        'bmi', 'updrs1_score', 'bjlot', 'scopa', 'upsit_pctl'\n",
    "    ],\n",
    "    'genetic': ['apoe_e4', 'subgroup_gba', 'subgroup_lrrk2', 'subgroup_prkn'],\n",
    "    'biomarkers': ['urate', 'csfsaa_positive_lbd_like', 'csfsaa_positive_msa_like', 'csfsaa_inconclusive'],\n",
    "    'datscan': ['mia_caudate_l', 'mia_caudate_r', 'mia_putamen_l', 'mia_putamen_r'],\n",
    "    'mri': [f'mri_pc{i+1}' for i in range(10)]\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "REGRESSION_TARGET = 'moca_slope_iqr_cleaned'\n",
    "SURVIVAL_TIME_TARGET = 'time_to_hy3_plus'\n",
    "SURVIVAL_EVENT_TARGET = 'event_occurred'\n",
    "ALL_MODALITIES = ['demographic_clinical', 'genetic', 'biomarkers', 'datscan', 'mri']\n",
    "\n",
    "# Baseline performance\n",
    "BASELINE_PERFORMANCE = {\n",
    "    'regression_r2': 0.21,\n",
    "    'survival_c_index': 0.83,\n",
    "    'regression_r2_baseline': 0.0,\n",
    "    'survival_c_index_baseline': 0.5\n",
    "}\n",
    "\n",
    "# Hyperparameter ranges\n",
    "N_RANDOM_SEARCH = 5000  # Adjust as needed\n",
    "EPOCHS = 1000\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "HYPERPARAMETER_RANGES = {\n",
    "    'learning_rate': [0.0001, 0.0005, 0.001, 0.002],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'hidden_dim': [64, 128, 256],\n",
    "    'alpha_regression': [0.01, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0],\n",
    "    'alpha_survival': [0.01, 0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0],\n",
    "    'l2_reg': [0.0, 0.0001, 0.001, 0.01],\n",
    "    'dropout_rate': [0.2, 0.4],\n",
    "    'n_layers': [2, 3],\n",
    "    'activation': ['relu'],\n",
    "    'batch_norm': [True, False],\n",
    "    'early_stopping_patience': [80]\n",
    "}\n",
    "\n",
    "features_to_standardize = [\n",
    "    'age', 'fampd', 'educyrs', 'apoe_e4', 'urate', 'mia_caudate_l', 'mia_caudate_r', \n",
    "    'mia_putamen_l', 'mia_putamen_r', 'duration_yrs', 'ledd', 'mseadlg', 'hy', \n",
    "    'hvltrdly', 'lns', 'vltanim', 'quip', 'ess', 'pigd', 'updrs2_score', \n",
    "    'updrs3_score', 'sdmtotal', 'stai', 'moca', 'rem', 'gds', 'bmi', \n",
    "    'updrs1_score', 'bjlot', 'scopa', 'upsit_pctl', 'mri_pc1', 'mri_pc2', \n",
    "    'mri_pc3', 'mri_pc4', 'mri_pc5', 'mri_pc6', 'mri_pc7', 'mri_pc8', \n",
    "    'mri_pc9', 'mri_pc10'\n",
    "]\n",
    "\n",
    "print(f\"ğŸ¯ Targets: {REGRESSION_TARGET} + {SURVIVAL_TIME_TARGET}\")\n",
    "print(f\"ğŸ“Š Total features: {sum(len(modality_groups[mod]) for mod in ALL_MODALITIES)}\")\n",
    "print(f\"âš™ï¸ Experiments: {N_RANDOM_SEARCH}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('results/mtl', exist_ok=True)\n",
    "os.makedirs('results/models/mtl', exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "# --- COMBINED SCORE CALCULATION ---\n",
    "\n",
    "def calculate_corrected_combined_score(test_r2, test_c_index, method='normalized_improvement'):\n",
    "    \"\"\"Calculate corrected combined score\"\"\"\n",
    "    \n",
    "    if method == 'normalized_improvement':\n",
    "        r2_baseline = BASELINE_PERFORMANCE['regression_r2_baseline']  # 0.0\n",
    "        c_index_baseline = BASELINE_PERFORMANCE['survival_c_index_baseline']  # 0.5\n",
    "        \n",
    "        # Calculate relative improvement\n",
    "        r2_improvement = (test_r2 - r2_baseline) / (1.0 - r2_baseline)\n",
    "        c_index_improvement = (test_c_index - c_index_baseline) / (1.0 - c_index_baseline)\n",
    "        \n",
    "        # Handle negative values\n",
    "        r2_improvement = max(0, r2_improvement)\n",
    "        c_index_improvement = max(0, c_index_improvement)\n",
    "        \n",
    "        # Equal weight average\n",
    "        combined_score = 0.5 * r2_improvement + 0.5 * c_index_improvement\n",
    "        \n",
    "        return combined_score, r2_improvement, c_index_improvement\n",
    "    \n",
    "    else:\n",
    "        # Simple method\n",
    "        return test_r2 + test_c_index, test_r2, test_c_index\n",
    "\n",
    "# --- PARAMETER GENERATOR ---\n",
    "\n",
    "def generate_random_params():\n",
    "    \"\"\"Generate random hyperparameter combination\"\"\"\n",
    "    return {\n",
    "        'learning_rate': random.choice(HYPERPARAMETER_RANGES['learning_rate']),\n",
    "        'batch_size': random.choice(HYPERPARAMETER_RANGES['batch_size']),\n",
    "        'hidden_dim': random.choice(HYPERPARAMETER_RANGES['hidden_dim']),\n",
    "        'alpha_regression': random.choice(HYPERPARAMETER_RANGES['alpha_regression']),\n",
    "        'alpha_survival': random.choice(HYPERPARAMETER_RANGES['alpha_survival']),\n",
    "        'l2_reg': random.choice(HYPERPARAMETER_RANGES['l2_reg']),\n",
    "        'dropout_rate': random.choice(HYPERPARAMETER_RANGES['dropout_rate']),\n",
    "        'n_layers': random.choice(HYPERPARAMETER_RANGES['n_layers']),\n",
    "        'activation': random.choice(HYPERPARAMETER_RANGES['activation']),\n",
    "        'batch_norm': random.choice(HYPERPARAMETER_RANGES['batch_norm']),\n",
    "        'early_stopping_patience': random.choice(HYPERPARAMETER_RANGES['early_stopping_patience'])\n",
    "    }\n",
    "\n",
    "# --- DATA PREPROCESSING ---\n",
    "\n",
    "class CompleteMTLDataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = None\n",
    "        self.fitted = False\n",
    "        \n",
    "    def prepare_and_split_data(self, df, test_size=0.2, random_state=42):\n",
    "        \"\"\"Complete data processing with train/test split preservation\"\"\"\n",
    "        try:\n",
    "            # 1. Get feature list\n",
    "            feature_list = []\n",
    "            for modality in ALL_MODALITIES:\n",
    "                if modality in modality_groups:\n",
    "                    feature_list.extend(modality_groups[modality])\n",
    "            \n",
    "            available_features = [f for f in feature_list if f in df.columns]\n",
    "            \n",
    "            # 2. Create complete dataset\n",
    "            required_cols = available_features + [REGRESSION_TARGET, SURVIVAL_TIME_TARGET, SURVIVAL_EVENT_TARGET]\n",
    "            complete_data = df[required_cols].copy()\n",
    "            \n",
    "            # 3. Convert targets to numeric\n",
    "            for col in [REGRESSION_TARGET, SURVIVAL_TIME_TARGET, SURVIVAL_EVENT_TARGET]:\n",
    "                complete_data[col] = pd.to_numeric(complete_data[col], errors='coerce')\n",
    "            \n",
    "            # 4. Remove NaN targets\n",
    "            complete_data = complete_data.dropna(subset=[REGRESSION_TARGET, SURVIVAL_TIME_TARGET, SURVIVAL_EVENT_TARGET])\n",
    "            \n",
    "            if len(complete_data) < 100:\n",
    "                print(f\"   âŒ Insufficient samples: {len(complete_data)}\")\n",
    "                return None\n",
    "            \n",
    "            # 5. Stratified split\n",
    "            try:\n",
    "                y_quartiles = pd.qcut(complete_data[REGRESSION_TARGET], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'], duplicates='drop')\n",
    "                train_idx, test_idx = train_test_split(\n",
    "                    range(len(complete_data)), \n",
    "                    test_size=test_size, \n",
    "                    random_state=random_state, \n",
    "                    stratify=y_quartiles\n",
    "                )\n",
    "            except:\n",
    "                train_idx, test_idx = train_test_split(\n",
    "                    range(len(complete_data)), \n",
    "                    test_size=test_size, \n",
    "                    random_state=random_state\n",
    "                )\n",
    "            \n",
    "            # 6. Split data\n",
    "            train_data = complete_data.iloc[train_idx].copy()\n",
    "            test_data = complete_data.iloc[test_idx].copy()\n",
    "            \n",
    "            # 7. Process features separately\n",
    "            X_train = self._process_features(train_data[available_features], fit_scaler=True)\n",
    "            X_test = self._process_features(test_data[available_features], fit_scaler=False)\n",
    "            \n",
    "            # 8. Return complete data splits\n",
    "            return {\n",
    "                'X_train': X_train,\n",
    "                'X_test': X_test,\n",
    "                'y_reg_train': train_data[REGRESSION_TARGET],\n",
    "                'y_reg_test': test_data[REGRESSION_TARGET],\n",
    "                'y_time_train': train_data[SURVIVAL_TIME_TARGET],\n",
    "                'y_time_test': test_data[SURVIVAL_TIME_TARGET],\n",
    "                'y_event_train': train_data[SURVIVAL_EVENT_TARGET],\n",
    "                'y_event_test': test_data[SURVIVAL_EVENT_TARGET],\n",
    "                'feature_names': available_features,\n",
    "                'train_indices': train_idx,\n",
    "                'test_indices': test_idx,\n",
    "                'complete_data': complete_data\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Data preparation error: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _process_features(self, X, fit_scaler=True):\n",
    "        \"\"\"Process features: boolean conversion + standardization\"\"\"\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # Boolean conversion\n",
    "        for col in X_processed.columns:\n",
    "            if X_processed[col].dtype == 'bool':\n",
    "                X_processed[col] = X_processed[col].astype(int)\n",
    "            elif X_processed[col].dtype == 'object':\n",
    "                X_processed[col] = pd.to_numeric(X_processed[col], errors='coerce')\n",
    "        \n",
    "        X_processed = X_processed.fillna(0)\n",
    "        \n",
    "        # Standardization\n",
    "        features_to_scale = [f for f in features_to_standardize if f in X_processed.columns]\n",
    "        \n",
    "        if features_to_scale:\n",
    "            if fit_scaler:\n",
    "                X_processed[features_to_scale] = self.scaler.fit_transform(X_processed[features_to_scale])\n",
    "                self.fitted = True\n",
    "            elif self.fitted:\n",
    "                X_processed[features_to_scale] = self.scaler.transform(X_processed[features_to_scale])\n",
    "        \n",
    "        return X_processed.astype(np.float32)\n",
    "\n",
    "# --- PYTORCH COMPONENTS ---\n",
    "\n",
    "class MTLDataset(Dataset):\n",
    "    def __init__(self, X, y_regression, y_time, y_event):\n",
    "        # Ensure data is numeric\n",
    "        X_processed = X.copy()\n",
    "        for col in X_processed.columns:\n",
    "            if X_processed[col].dtype == 'bool':\n",
    "                X_processed[col] = X_processed[col].astype(int)\n",
    "            elif X_processed[col].dtype == 'object':\n",
    "                X_processed[col] = pd.to_numeric(X_processed[col], errors='coerce')\n",
    "        \n",
    "        X_processed = X_processed.fillna(0).astype(np.float32)\n",
    "        \n",
    "        self.X = torch.tensor(X_processed.values, dtype=torch.float32)\n",
    "        self.y_regression = torch.tensor(y_regression.values.astype(np.float32), dtype=torch.float32).view(-1, 1)\n",
    "        self.y_time = torch.tensor(y_time.values.astype(np.float32), dtype=torch.float32).view(-1, 1)\n",
    "        self.y_event = torch.tensor(y_event.values.astype(np.float32), dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_regression[idx], self.y_time[idx], self.y_event[idx]\n",
    "\n",
    "class CompleteMTLNet(nn.Module):\n",
    "    def __init__(self, n_features, params):\n",
    "        super(CompleteMTLNet, self).__init__()\n",
    "        \n",
    "        hidden_dim = params['hidden_dim']\n",
    "        n_layers = params['n_layers']\n",
    "        dropout_rate = params['dropout_rate']\n",
    "        batch_norm = params['batch_norm']\n",
    "        \n",
    "        self.activation_fn = nn.ReLU()\n",
    "        \n",
    "        # Shared encoder\n",
    "        layers = []\n",
    "        input_dim = n_features\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(self.activation_fn)\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        self.shared_encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            self.activation_fn,\n",
    "            nn.Dropout(dropout_rate / 2),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.survival_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            self.activation_fn,\n",
    "            nn.Dropout(dropout_rate / 2),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_features = self.shared_encoder(x)\n",
    "        regression_output = self.regression_head(shared_features)\n",
    "        survival_output = self.survival_head(shared_features)\n",
    "        return regression_output, survival_output\n",
    "\n",
    "def enhanced_cox_loss(log_hazard, durations, events, eps=1e-8):\n",
    "    \"\"\"Enhanced Cox loss with numerical stability\"\"\"\n",
    "    if torch.sum(events) == 0:\n",
    "        return torch.tensor(0.0, requires_grad=True, device=log_hazard.device)\n",
    "    \n",
    "    sorted_indices = torch.argsort(durations.view(-1), descending=True)\n",
    "    log_hazard_sorted = log_hazard.view(-1)[sorted_indices]\n",
    "    events_sorted = events.view(-1)[sorted_indices]\n",
    "    \n",
    "    exp_hazard = torch.exp(torch.clamp(log_hazard_sorted, -20, 20))\n",
    "    cumsum_exp = torch.cumsum(exp_hazard, dim=0)\n",
    "    log_risk_set = torch.log(cumsum_exp + eps)\n",
    "    \n",
    "    event_mask = events_sorted == 1\n",
    "    if torch.sum(event_mask) == 0:\n",
    "        return torch.tensor(0.0, requires_grad=True, device=log_hazard.device)\n",
    "    \n",
    "    pll = torch.sum(log_hazard_sorted[event_mask] - log_risk_set[event_mask])\n",
    "    return -pll\n",
    "\n",
    "def compute_l2_regularization(model, l2_reg):\n",
    "    \"\"\"Compute L2 regularization term\"\"\"\n",
    "    if l2_reg == 0:\n",
    "        return torch.tensor(0.0, device=next(model.parameters()).device)\n",
    "    \n",
    "    l2_loss = torch.tensor(0.0, device=next(model.parameters()).device)\n",
    "    for param in model.parameters():\n",
    "        l2_loss += torch.norm(param, p=2) ** 2\n",
    "    return l2_reg * l2_loss\n",
    "\n",
    "def compute_balanced_mtl_loss(reg_pred, surv_pred, reg_target, time_target, event_target, \n",
    "                             alpha_regression, alpha_survival, model, l2_reg):\n",
    "    \"\"\"Balanced MTL loss function\"\"\"\n",
    "    # Regression loss\n",
    "    mse_loss = nn.MSELoss()(reg_pred, reg_target)\n",
    "    \n",
    "    # Survival loss\n",
    "    cox_loss = enhanced_cox_loss(surv_pred, time_target, event_target)\n",
    "    \n",
    "    # L2 regularization\n",
    "    l2_loss = compute_l2_regularization(model, l2_reg)\n",
    "    \n",
    "    # Dynamic balance\n",
    "    if not torch.isnan(mse_loss) and not torch.isnan(cox_loss) and mse_loss > 0 and cox_loss > 0:\n",
    "        mse_scale = mse_loss.detach()\n",
    "        cox_scale = cox_loss.detach()\n",
    "        \n",
    "        if mse_scale > 0 and cox_scale > 0:\n",
    "            scale_factor = mse_scale / cox_scale\n",
    "            adjusted_alpha_survival = alpha_survival * max(1.0, scale_factor / 10.0)\n",
    "            total_loss = alpha_regression * mse_loss + adjusted_alpha_survival * cox_loss + l2_loss\n",
    "        else:\n",
    "            total_loss = alpha_regression * mse_loss + alpha_survival * cox_loss + l2_loss\n",
    "    else:\n",
    "        total_loss = alpha_regression * mse_loss + alpha_survival * cox_loss + l2_loss\n",
    "    \n",
    "    return total_loss, mse_loss, cox_loss, l2_loss\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=50, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            return self.counter >= self.patience\n",
    "\n",
    "print(\"âœ… Complete components defined!\")\n",
    "\n",
    "# --- COMPLETE MTL PIPELINE ---\n",
    "\n",
    "class CompleteMTLPipeline:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.preprocessor = CompleteMTLDataPreprocessor()\n",
    "        \n",
    "    def train_and_evaluate(self, df):\n",
    "        \"\"\"Train and evaluate complete MTL model with data preservation\"\"\"\n",
    "        # Use complete data processing\n",
    "        data = self.preprocessor.prepare_and_split_data(df)\n",
    "        if data is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Create datasets\n",
    "            train_dataset = MTLDataset(\n",
    "                data['X_train'], data['y_reg_train'], \n",
    "                data['y_time_train'], data['y_event_train']\n",
    "            )\n",
    "            \n",
    "            train_loader = DataLoader(\n",
    "                train_dataset, \n",
    "                batch_size=min(self.params['batch_size'], len(train_dataset)), \n",
    "                shuffle=True,\n",
    "                drop_last=False\n",
    "            )\n",
    "            \n",
    "            # Initialize model\n",
    "            model = CompleteMTLNet(\n",
    "                n_features=train_dataset.X.shape[1], \n",
    "                params=self.params\n",
    "            ).to(self.device)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.parameters(), \n",
    "                lr=self.params['learning_rate']\n",
    "            )\n",
    "            \n",
    "            early_stopping = EarlyStopping(patience=self.params['early_stopping_patience'])\n",
    "            \n",
    "            # Training loop\n",
    "            model.train()\n",
    "            train_losses = []\n",
    "            \n",
    "            for epoch in range(EPOCHS):\n",
    "                epoch_losses = []\n",
    "                epoch_mse_losses = []\n",
    "                epoch_cox_losses = []\n",
    "                \n",
    "                for X_batch, y_reg_batch, y_time_batch, y_event_batch in train_loader:\n",
    "                    X_batch = X_batch.to(self.device)\n",
    "                    y_reg_batch = y_reg_batch.to(self.device)\n",
    "                    y_time_batch = y_time_batch.to(self.device)\n",
    "                    y_event_batch = y_event_batch.to(self.device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    reg_pred, surv_pred = model(X_batch)\n",
    "                    \n",
    "                    # Compute balanced MTL loss\n",
    "                    total_loss, mse_loss, cox_loss, l2_loss = compute_balanced_mtl_loss(\n",
    "                        reg_pred, surv_pred, y_reg_batch, y_time_batch, y_event_batch,\n",
    "                        self.params['alpha_regression'], self.params['alpha_survival'], \n",
    "                        model, self.params['l2_reg']\n",
    "                    )\n",
    "                    \n",
    "                    total_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_losses.append(total_loss.item())\n",
    "                    epoch_mse_losses.append(mse_loss.item())\n",
    "                    epoch_cox_losses.append(cox_loss.item())\n",
    "                \n",
    "                if epoch_losses:\n",
    "                    avg_epoch_loss = np.mean(epoch_losses)\n",
    "                    train_losses.append(avg_epoch_loss)\n",
    "                    \n",
    "                    if early_stopping(avg_epoch_loss):\n",
    "                        break\n",
    "            \n",
    "            # Evaluation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_test_processed = data['X_test'].copy()\n",
    "                \n",
    "                for col in X_test_processed.columns:\n",
    "                    if X_test_processed[col].dtype == 'bool':\n",
    "                        X_test_processed[col] = X_test_processed[col].astype(int)\n",
    "                    elif X_test_processed[col].dtype == 'object':\n",
    "                        X_test_processed[col] = pd.to_numeric(X_test_processed[col], errors='coerce')\n",
    "                \n",
    "                X_test_processed = X_test_processed.fillna(0).astype(np.float32)\n",
    "                X_test_tensor = torch.tensor(X_test_processed.values, dtype=torch.float32).to(self.device)\n",
    "                \n",
    "                reg_pred_test, surv_pred_test = model(X_test_tensor)\n",
    "                \n",
    "                # Regression metrics\n",
    "                reg_pred_test_np = reg_pred_test.cpu().numpy().flatten()\n",
    "                test_r2 = r2_score(data['y_reg_test'], reg_pred_test_np)\n",
    "                test_mae = mean_absolute_error(data['y_reg_test'], reg_pred_test_np)\n",
    "                test_rmse = np.sqrt(mean_squared_error(data['y_reg_test'], reg_pred_test_np))\n",
    "                \n",
    "                # Survival metrics\n",
    "                surv_pred_test_np = surv_pred_test.cpu().numpy().flatten()\n",
    "                test_c_index = concordance_index(\n",
    "                    data['y_time_test'], \n",
    "                    -surv_pred_test_np,\n",
    "                    data['y_event_test']\n",
    "                )\n",
    "            \n",
    "            # Calculate corrected combined score\n",
    "            combined_score, r2_normalized, c_index_normalized = calculate_corrected_combined_score(\n",
    "                test_r2, test_c_index, method='normalized_improvement'\n",
    "            )\n",
    "            \n",
    "            # Calculate improvements\n",
    "            r2_improvement = test_r2 - BASELINE_PERFORMANCE['regression_r2']\n",
    "            c_index_improvement = test_c_index - BASELINE_PERFORMANCE['survival_c_index']\n",
    "            \n",
    "            return {\n",
    "                'n_features': len(data['feature_names']),\n",
    "                'n_train': len(data['X_train']),\n",
    "                'n_test': len(data['X_test']),\n",
    "                'test_r2': test_r2,\n",
    "                'test_mae': test_mae,\n",
    "                'test_rmse': test_rmse,\n",
    "                'test_c_index': test_c_index,\n",
    "                'r2_improvement': r2_improvement,\n",
    "                'c_index_improvement': c_index_improvement,\n",
    "                'combined_score': combined_score,\n",
    "                'combined_score_simple': test_r2 + test_c_index,\n",
    "                'r2_normalized': r2_normalized,\n",
    "                'c_index_normalized': c_index_normalized,\n",
    "                'epochs_trained': len(train_losses),\n",
    "                'final_train_loss': train_losses[-1] if train_losses else np.nan,\n",
    "                'avg_mse_loss': np.mean(epoch_mse_losses) if epoch_mse_losses else np.nan,\n",
    "                'avg_cox_loss': np.mean(epoch_cox_losses) if epoch_cox_losses else np.nan,\n",
    "                'model': model,\n",
    "                'scaler': self.preprocessor.scaler,\n",
    "                'feature_names': data['feature_names'],\n",
    "                # ğŸ”¥ KEY: Preserve training data for accurate evaluation\n",
    "                'training_data': {\n",
    "                    'X_train': data['X_train'],\n",
    "                    'X_test': data['X_test'],\n",
    "                    'y_reg_train': data['y_reg_train'],\n",
    "                    'y_reg_test': data['y_reg_test'],\n",
    "                    'y_time_train': data['y_time_train'],\n",
    "                    'y_time_test': data['y_time_test'],\n",
    "                    'y_event_train': data['y_event_train'],\n",
    "                    'y_event_test': data['y_event_test'],\n",
    "                    'train_indices': data['train_indices'],\n",
    "                    'test_indices': data['test_indices'],\n",
    "                    'reg_predictions': reg_pred_test_np,\n",
    "                    'surv_predictions': surv_pred_test_np\n",
    "                },\n",
    "                **self.params\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      Training error: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "def run_complete_mtl_experiment(df):\n",
    "    \"\"\"Run complete MTL experiment with data preservation\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ STARTING COMPLETE MTL EXPERIMENT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    all_results = []\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(f\"ğŸ“Š Experiments: {N_RANDOM_SEARCH}\")\n",
    "    print(f\"ğŸ“… Epochs per experiment: {EPOCHS}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ BASELINE COMPARISON:\")\n",
    "    print(f\"   â€¢ Regression RÂ²: {BASELINE_PERFORMANCE['regression_r2']:.3f}\")\n",
    "    print(f\"   â€¢ Survival C-index: {BASELINE_PERFORMANCE['survival_c_index']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”§ COMPLETE MTL FEATURES:\")\n",
    "    print(f\"   â€¢ Training data preservation\")\n",
    "    print(f\"   â€¢ Corrected data processing\")\n",
    "    print(f\"   â€¢ Balanced loss function\")\n",
    "    print(f\"   â€¢ Normalized combined score\")\n",
    "    \n",
    "    # Track best results\n",
    "    best_combined_score = -np.inf\n",
    "    best_result = None\n",
    "    successful_experiments = 0\n",
    "    \n",
    "    for experiment_id in range(1, N_RANDOM_SEARCH + 1):\n",
    "        print(f\"\\n--- Experiment {experiment_id}/{N_RANDOM_SEARCH} ---\")\n",
    "        \n",
    "        params = generate_random_params()\n",
    "        print(f\"Params: lr={params['learning_rate']}, bs={params['batch_size']}, \"\n",
    "              f\"hd={params['hidden_dim']}, Î±r={params['alpha_regression']}, Î±s={params['alpha_survival']}\")\n",
    "        \n",
    "        try:\n",
    "            pipeline = CompleteMTLPipeline(params)\n",
    "            result = pipeline.train_and_evaluate(df)\n",
    "            \n",
    "            if result is not None:\n",
    "                successful_experiments += 1\n",
    "                \n",
    "                result.update({\n",
    "                    'timestamp': timestamp,\n",
    "                    'experiment_id': experiment_id\n",
    "                })\n",
    "                \n",
    "                # Display key metrics\n",
    "                print(f\"ğŸ“Š RÂ²: {result['test_r2']:.4f} | C-idx: {result['test_c_index']:.4f}\")\n",
    "                print(f\"ğŸ“Š RÂ² Î”: {result['r2_improvement']:+.4f} | C-idx Î”: {result['c_index_improvement']:+.4f}\")\n",
    "                print(f\"ğŸ“Š Combined (norm): {result['combined_score']:.4f} | Combined (simple): {result['combined_score_simple']:.4f}\")\n",
    "                print(f\"ğŸ“Š Epochs: {result['epochs_trained']}\")\n",
    "                \n",
    "                all_results.append(result)\n",
    "                \n",
    "                # Use normalized combined score for best model selection\n",
    "                if result['combined_score'] > best_combined_score:\n",
    "                    best_combined_score = result['combined_score']\n",
    "                    best_result = result.copy()\n",
    "                    \n",
    "                    # ğŸ”¥ Save best model with complete training data\n",
    "                    model_path = f\"results/models/mtl/best_complete_mtl_{timestamp}.pt\"\n",
    "                    torch.save({\n",
    "                        'model_state_dict': result['model'].state_dict(),\n",
    "                        'params': params,\n",
    "                        'metrics': {\n",
    "                            'test_r2': result['test_r2'],\n",
    "                            'test_c_index': result['test_c_index'],\n",
    "                            'combined_score': result['combined_score'],\n",
    "                            'combined_score_simple': result['combined_score_simple'],\n",
    "                            'r2_improvement': result['r2_improvement'],\n",
    "                            'c_index_improvement': result['c_index_improvement']\n",
    "                        },\n",
    "                        'feature_names': result['feature_names'],\n",
    "                        # ğŸ”¥ KEY: Save complete training data for accurate evaluation\n",
    "                        'training_data': result['training_data'],\n",
    "                        'scaler_params': {\n",
    "                            'scaler_mean': result['scaler'].mean_ if hasattr(result['scaler'], 'mean_') else None,\n",
    "                            'scaler_scale': result['scaler'].scale_ if hasattr(result['scaler'], 'scale_') else None,\n",
    "                            'features_to_standardize': [f for f in features_to_standardize if f in result['feature_names']]\n",
    "                        },\n",
    "                        'architecture_config': {\n",
    "                            'input_dim': len(result['feature_names']),\n",
    "                            'hidden_dim': params['hidden_dim'],\n",
    "                            'dropout_rate': params['dropout_rate'],\n",
    "                            'n_layers': params['n_layers'],\n",
    "                            'batch_norm': params['batch_norm']\n",
    "                        }\n",
    "                    }, model_path)\n",
    "                    print(f\"ğŸ’¾ New best model + training data saved! {timestamp}\")\n",
    "            \n",
    "            else:\n",
    "                print(\"âš ï¸ Training failed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nâœ… Completed: {successful_experiments}/{N_RANDOM_SEARCH} successful\")\n",
    "    return all_results, best_result, timestamp\n",
    "\n",
    "# Execute experiment\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    data_path = './data/'\n",
    "    curated_mri_vif_df = pd.read_csv(data_path + '3_baseline_vif.csv')\n",
    "    curated_mri_vif_df = curated_mri_vif_df.dropna()\n",
    "    \n",
    "    print(\"ğŸ¯ Starting Complete MTL Experiment\")\n",
    "    print(f\"Data shape: {curated_mri_vif_df.shape}\")\n",
    "    \n",
    "    # Run experiment\n",
    "    all_results, best_result, timestamp = run_complete_mtl_experiment(curated_mri_vif_df)\n",
    "    \n",
    "    # --- RESULTS ANALYSIS ---\n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ† COMPLETE MTL EXPERIMENT RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Best model\n",
    "        if best_result:\n",
    "            print(f\"\\nğŸ¥‡ BEST COMPLETE MTL MODEL:\")\n",
    "            print(f\"   â€¢ Test RÂ²: {best_result['test_r2']:.4f} (Î” = {best_result['r2_improvement']:+.4f})\")\n",
    "            print(f\"   â€¢ Test C-index: {best_result['test_c_index']:.4f} (Î” = {best_result['c_index_improvement']:+.4f})\")\n",
    "            print(f\"   â€¢ Combined Score (normalized): {best_result['combined_score']:.4f}\")\n",
    "            print(f\"   â€¢ Combined Score (simple): {best_result['combined_score_simple']:.4f}\")\n",
    "            print(f\"   â€¢ RÂ² normalized: {best_result['r2_normalized']:.4f}\")\n",
    "            print(f\"   â€¢ C-index normalized: {best_result['c_index_normalized']:.4f}\")\n",
    "            print(f\"   â€¢ Test samples: {best_result['n_test']}\")\n",
    "            print(f\"   â€¢ Hyperparameters:\")\n",
    "            print(f\"     - Î±_regression: {best_result['alpha_regression']}\")\n",
    "            print(f\"     - Î±_survival: {best_result['alpha_survival']}\")\n",
    "            print(f\"     - Learning rate: {best_result['learning_rate']}\")\n",
    "            print(f\"     - Hidden dim: {best_result['hidden_dim']}\")\n",
    "            print(f\"     - L2 reg: {best_result['l2_reg']}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\nğŸ“Š EXPERIMENT SUMMARY:\")\n",
    "        print(f\"   â€¢ Mean RÂ²: {results_df['test_r2'].mean():.4f} Â± {results_df['test_r2'].std():.4f}\")\n",
    "        print(f\"   â€¢ Mean C-index: {results_df['test_c_index'].mean():.4f} Â± {results_df['test_c_index'].std():.4f}\")\n",
    "        print(f\"   â€¢ Mean Combined (norm): {results_df['combined_score'].mean():.4f} Â± {results_df['combined_score'].std():.4f}\")\n",
    "        print(f\"   â€¢ Mean Combined (simple): {results_df['combined_score_simple'].mean():.4f} Â± {results_df['combined_score_simple'].std():.4f}\")\n",
    "        \n",
    "        # Improvement analysis\n",
    "        r2_improvements = results_df['r2_improvement'] > 0\n",
    "        c_index_improvements = results_df['c_index_improvement'] > 0\n",
    "        both_improvements = r2_improvements & c_index_improvements\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ IMPROVEMENT ANALYSIS:\")\n",
    "        print(f\"   â€¢ RÂ² improvements: {r2_improvements.sum()}/{len(results_df)} ({100*r2_improvements.mean():.1f}%)\")\n",
    "        print(f\"   â€¢ C-index improvements: {c_index_improvements.sum()}/{len(results_df)} ({100*c_index_improvements.mean():.1f}%)\")\n",
    "        print(f\"   â€¢ Both improvements: {both_improvements.sum()}/{len(results_df)} ({100*both_improvements.mean():.1f}%)\")\n",
    "        \n",
    "        # Top 10 models\n",
    "        top_10 = results_df.nlargest(10, 'combined_score')[\n",
    "            ['test_r2', 'test_c_index', 'combined_score', 'combined_score_simple', \n",
    "             'r2_improvement', 'c_index_improvement', 'alpha_regression', 'alpha_survival', \n",
    "             'learning_rate', 'hidden_dim', 'l2_reg']\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nğŸ† TOP 10 MODELS (by normalized combined score):\")\n",
    "        print(top_10.round(4).to_string())\n",
    "        \n",
    "        # Save results\n",
    "        results_file = f'results/mtl/complete_mtl_results_{timestamp}.csv'\n",
    "        # Remove training_data from DataFrame for CSV export\n",
    "        results_df_clean = results_df.drop(columns=['model', 'scaler', 'training_data'], errors='ignore')\n",
    "        results_df_clean.to_csv(results_file, index=False)\n",
    "        \n",
    "        # Final conclusion\n",
    "        print(f\"\\nğŸ¯ FINAL CONCLUSION:\")\n",
    "        if best_result:\n",
    "            if best_result['r2_improvement'] > 0 and best_result['c_index_improvement'] > 0:\n",
    "                print(\"âœ… COMPLETE MTL OUTPERFORMS on BOTH tasks!\")\n",
    "            elif best_result['r2_improvement'] > 0:\n",
    "                print(\"âš ï¸ COMPLETE MTL OUTPERFORMS on regression only\")\n",
    "            elif best_result['c_index_improvement'] > 0:\n",
    "                print(\"âš ï¸ COMPLETE MTL OUTPERFORMS on survival only\")\n",
    "            else:\n",
    "                print(\"âŒ COMPLETE MTL does NOT outperform baselines\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Results saved: {results_file}\")\n",
    "        \n",
    "        # Training data verification\n",
    "        if best_result and 'training_data' in best_result:\n",
    "            training_data = best_result['training_data']\n",
    "            print(f\"\\nğŸ“Š TRAINING DATA PRESERVED:\")\n",
    "            print(f\"   â€¢ Training samples: {len(training_data['X_train'])}\")\n",
    "            print(f\"   â€¢ Test samples: {len(training_data['X_test'])}\")\n",
    "            print(f\"   â€¢ Features: {len(best_result['feature_names'])}\")\n",
    "            print(f\"   â€¢ Predictions available: regression & survival\")\n",
    "        \n",
    "    print(\"\\n\" + \"ğŸ‰\" * 30)\n",
    "    print(\"COMPLETE MTL EXPERIMENT FINISHED\")\n",
    "    print(\"ğŸ‰\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff1518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error loading model: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy._core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy._core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "def inspect_model_file(model_path):\n",
    "    import torch\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        print(f\"ğŸ” Model file contents:\")\n",
    "        print(f\"Keys in checkpoint: {list(checkpoint.keys())}\")\n",
    "        \n",
    "        for key, value in checkpoint.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"  {key}: {list(value.keys())}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {type(value)}\")\n",
    "                \n",
    "        return checkpoint\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {e}\")\n",
    "        return None\n",
    "    \n",
    "model_path = \"./results/models/mtl/best_final_mtl.pt\"\n",
    "checkpoint = inspect_model_file(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DIAGNOSING FEATURE MISMATCH\n",
      "============================================================\n",
      "âœ… Model expects: 53 features\n",
      "\n",
      "ğŸ“Š THEORETICAL FEATURES (ALL_MODALITIES):\n",
      "   â€¢ demographic_clinical: 31 features\n",
      "     ['age', 'fampd', 'race_black', 'race_asian', 'race_other']...\n",
      "   â€¢ genetic: 4 features\n",
      "     ['apoe_e4', 'subgroup_gba', 'subgroup_lrrk2', 'subgroup_prkn']\n",
      "   â€¢ biomarkers: 4 features\n",
      "     ['urate', 'csfsaa_positive_lbd_like', 'csfsaa_positive_msa_like', 'csfsaa_inconclusive']\n",
      "   â€¢ datscan: 4 features\n",
      "     ['mia_caudate_l', 'mia_caudate_r', 'mia_putamen_l', 'mia_putamen_r']\n",
      "   â€¢ mri: 10 features\n",
      "     ['mri_pc1', 'mri_pc2', 'mri_pc3', 'mri_pc4', 'mri_pc5']...\n",
      "\n",
      "   ğŸ“‹ Total theoretical features: 53\n",
      "\n",
      "ğŸ“ˆ ACTUAL FEATURES IN DATAFRAME:\n",
      "   â€¢ Available in df: 53 features\n",
      "   â€¢ Missing from df: 0 features\n",
      "\n",
      "ğŸ” FEATURE TYPE ANALYSIS:\n",
      "   â€¢ Numeric features: 42\n",
      "   â€¢ Non-numeric features: 11\n",
      "   â€¢ Non-numeric: ['race_black', 'race_asian', 'race_other', 'domside_left', 'domside_symmetric', 'subgroup_gba', 'subgroup_lrrk2', 'subgroup_prkn', 'csfsaa_positive_lbd_like', 'csfsaa_positive_msa_like', 'csfsaa_inconclusive']\n",
      "     - race_black: dtype=bool, unique=2\n",
      "       Sample values: [False, True]\n",
      "     - race_asian: dtype=bool, unique=2\n",
      "       Sample values: [False, True]\n",
      "     - race_other: dtype=bool, unique=2\n",
      "       Sample values: [False, True]\n",
      "     - domside_left: dtype=bool, unique=2\n",
      "       Sample values: [True, False]\n",
      "     - domside_symmetric: dtype=bool, unique=2\n",
      "       Sample values: [False, True]\n",
      "\n",
      "ğŸ“Š FINAL SUMMARY:\n",
      "   â€¢ Model expects: 53 features\n",
      "   â€¢ Theoretical total: 53 features\n",
      "   â€¢ Actually available: 53 features\n",
      "   â€¢ Numeric available: 42 features\n",
      "   â€¢ Shortfall: 11 features\n",
      "\n",
      "ğŸ’¡ RECOMMENDATIONS:\n",
      "   2. Convert non-numeric features to numeric\n",
      "   3. Use feature padding (add zeros) to match expected dimensions\n",
      "   4. Or retrain model with available features only\n"
     ]
    }
   ],
   "source": [
    "def diagnose_feature_mismatch(model_path, df):\n",
    "    \"\"\"\n",
    "    Detailed diagnosis of feature mismatch issues\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"ğŸ” DIAGNOSING FEATURE MISMATCH\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Load model file\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        expected_input_dim = checkpoint['architecture_config']['input_dim']\n",
    "        print(f\"âœ… Model expects: {expected_input_dim} features\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 2. Get theoretical ALL_MODALITIES feature list\n",
    "    print(f\"\\nğŸ“Š THEORETICAL FEATURES (ALL_MODALITIES):\")\n",
    "    \n",
    "    all_theoretical_features = []\n",
    "    for modality in ALL_MODALITIES:\n",
    "        if modality in modality_groups:\n",
    "            features = modality_groups[modality]\n",
    "            all_theoretical_features.extend(features)\n",
    "            print(f\"   â€¢ {modality}: {len(features)} features\")\n",
    "            print(f\"     {features[:5]}...\" if len(features) > 5 else f\"     {features}\")\n",
    "    \n",
    "    print(f\"\\n   ğŸ“‹ Total theoretical features: {len(all_theoretical_features)}\")\n",
    "    \n",
    "    # 3. Check features actually present in dataframe\n",
    "    print(f\"\\nğŸ“ˆ ACTUAL FEATURES IN DATAFRAME:\")\n",
    "    available_features = [f for f in all_theoretical_features if f in df.columns]\n",
    "    missing_features = [f for f in all_theoretical_features if f not in df.columns]\n",
    "    \n",
    "    print(f\"   â€¢ Available in df: {len(available_features)} features\")\n",
    "    print(f\"   â€¢ Missing from df: {len(missing_features)} features\")\n",
    "    \n",
    "    # 4. Display missing features\n",
    "    if missing_features:\n",
    "        print(f\"\\nâŒ MISSING FEATURES:\")\n",
    "        for modality in ALL_MODALITIES:\n",
    "            modality_features = modality_groups[modality]\n",
    "            missing_in_modality = [f for f in modality_features if f in missing_features]\n",
    "            if missing_in_modality:\n",
    "                print(f\"   â€¢ {modality} missing: {missing_in_modality}\")\n",
    "    \n",
    "    # 5. Check data type issues\n",
    "    print(f\"\\nğŸ” FEATURE TYPE ANALYSIS:\")\n",
    "    numeric_features = df[available_features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    non_numeric_features = df[available_features].select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"   â€¢ Numeric features: {len(numeric_features)}\")\n",
    "    print(f\"   â€¢ Non-numeric features: {len(non_numeric_features)}\")\n",
    "    \n",
    "    if non_numeric_features:\n",
    "        print(f\"   â€¢ Non-numeric: {non_numeric_features}\")\n",
    "        \n",
    "        # Check data types and samples of non-numeric features\n",
    "        for col in non_numeric_features[:5]:  # Only show first 5\n",
    "            print(f\"     - {col}: dtype={df[col].dtype}, unique={df[col].nunique()}\")\n",
    "            print(f\"       Sample values: {df[col].unique()[:3].tolist()}\")\n",
    "    \n",
    "    # 6. Calculate final available feature count\n",
    "    final_available = len(numeric_features)\n",
    "    print(f\"\\nğŸ“Š FINAL SUMMARY:\")\n",
    "    print(f\"   â€¢ Model expects: {expected_input_dim} features\")\n",
    "    print(f\"   â€¢ Theoretical total: {len(all_theoretical_features)} features\")\n",
    "    print(f\"   â€¢ Actually available: {len(available_features)} features\")\n",
    "    print(f\"   â€¢ Numeric available: {final_available} features\")\n",
    "    print(f\"   â€¢ Shortfall: {expected_input_dim - final_available} features\")\n",
    "    \n",
    "    # 7. Suggest solutions\n",
    "    print(f\"\\nğŸ’¡ RECOMMENDATIONS:\")\n",
    "    if missing_features:\n",
    "        print(f\"   1. Add missing features to dataframe or remove from model training\")\n",
    "    if non_numeric_features:\n",
    "        print(f\"   2. Convert non-numeric features to numeric\")\n",
    "    if final_available < expected_input_dim:\n",
    "        print(f\"   3. Use feature padding (add zeros) to match expected dimensions\")\n",
    "        print(f\"   4. Or retrain model with available features only\")\n",
    "    \n",
    "    # 8. Return detailed information\n",
    "    return {\n",
    "        'expected_features': expected_input_dim,\n",
    "        'theoretical_features': all_theoretical_features,\n",
    "        'available_features': available_features,\n",
    "        'missing_features': missing_features,\n",
    "        'numeric_features': numeric_features,\n",
    "        'non_numeric_features': non_numeric_features,\n",
    "        'final_count': final_available,\n",
    "        'shortfall': expected_input_dim - final_available\n",
    "    }\n",
    "\n",
    "# Run diagnosis\n",
    "model_path = \"results/models/rq4_mtl_experiment_20250909_161400.pth\"\n",
    "diagnosis = diagnose_feature_mismatch(model_path, curated_mri_vif_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
